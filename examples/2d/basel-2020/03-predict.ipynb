{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from EmbedSeg.utils.create_dicts import create_test_configs_dict\n",
    "from EmbedSeg.test import begin_evaluating\n",
    "from glob import glob\n",
    "import tifffile\n",
    "import matplotlib.pyplot as plt\n",
    "from EmbedSeg.utils.visualize import visualize_im_pred\n",
    "import os\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "today = date.today()\n",
    "today = today.strftime(\"%b-%d-%Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the path to the evaluation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../../data'\n",
    "project_name = 'basel-2020'\n",
    "print(\"Evaluation images shall be read from: {}\".format(os.path.join(data_dir, project_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify evaluation parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some hints:\n",
    "* `tta`: Setting this to True (default) would enable test-time augmentation. For the `basel-2020` dataset, **set this parameter to False**. \n",
    "* `ap_val`: This parameter (\"average precision value\") comes into action if ground truth segmentations exist for evaluation images, and allows to compare how good our predictions are versus the available ground truth segmentations.\n",
    "* `seed_thresh`: This parameter (\"seediness threshold\") allows considering only those pixels as potential instance-centres which have a seediness score greater than `seed_thresh`\n",
    "* `min_object_size`: This parameter (\"minimum object size\") allows considering only those predicted instances which have more interior pixels than `min_object_size` \n",
    "* `checkpoint_path`: This parameter provides the path to the trained model weights which you would like to use for evaluation\n",
    "* `save_dir`: This parameter specifies the path to the prediction instances. Equal to `static` by default.\n",
    "* `save_images`: If True, this saves predictions at `static/predictions/` \n",
    "* `save_results`: If True, this saves results at `static/results/`\n",
    "* `normalization_factor`: Set this to either 255 (for 8-bit raw-images) or 65535 (for 16-bit instance-images). This parameter should be **set the same value** as `normalization_factor` used while creating `train_dataset_dict` and `val_dataset_dict` during training.\n",
    "* `one_hot`: In case, ground truth segmentations exist for evaluation images, then set this parameter to `True` if the GT segmentations are present in a one-hot encoded style (i.e. each object is encoded as 1 in its own individual slice and 0 elsewhere) or `False`, otherwise\n",
    "\n",
    "In the cell after this one, a `test_configs` dictionary is generated from the parameters specified here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tta = False\n",
    "ap_val = 0.50\n",
    "seed_thresh = 0.90\n",
    "min_object_size = 36\n",
    "checkpoint_path = os.path.join('experiment', 'basel-2020-Jan-03-2021', 'best_iou_model.pth')\n",
    "save_dir = 'static'\n",
    "save_images = True\n",
    "save_results = True\n",
    "normalization_factor = 1\n",
    "one_hot = False\n",
    "grid_y = 1024\n",
    "grid_x = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(checkpoint_path):\n",
    "    print(\"Trained model weights found at : {}\".format(checkpoint_path))\n",
    "else:\n",
    "    print(\"Trained model weights were not found at the specified location!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create `test_configs` dictionary from the above-specified parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_configs = create_test_configs_dict(data_dir = os.path.join(data_dir, project_name),\n",
    "                                        checkpoint_path = checkpoint_path,\n",
    "                                        tta = tta, \n",
    "                                        ap_val = ap_val,\n",
    "                                        seed_thresh = seed_thresh, \n",
    "                                        min_object_size = min_object_size, \n",
    "                                        save_images = save_images,\n",
    "                                        save_results = save_results,\n",
    "                                        save_dir = save_dir,\n",
    "                                        normalization_factor = normalization_factor,\n",
    "                                        one_hot = one_hot,\n",
    "                                        grid_y = grid_y,\n",
    "                                        grid_x = grid_x,\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin Evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting `verbose` to True shows you Average Precision at IOU threshold specified by `ap_val` above for each individual image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib agg\n",
    "begin_evaluating(test_configs, verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on more unseen images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Specify below the path to the image which you would like to evaluate!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_numpy = tifffile.imread('/media/manan/Samsung_T51/Manan/EmbedSeg_RC/data/basel-2020/test/images/X_9.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two cells simply push the image to the GPU and load the model with the trained weights. Simply press <kbd>Shift</kbd> + <kbd>Enter</kbd> on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from EmbedSeg.models import get_model\n",
    "from EmbedSeg.utils.utils import Cluster\n",
    "device = torch.device(\"cuda:0\" if test_configs['cuda'] else \"cpu\")\n",
    "model = get_model(test_configs['model']['name'], test_configs['model']['kwargs'])\n",
    "model = torch.nn.DataParallel(model).to(device)\n",
    "state = torch.load(test_configs['checkpoint_path'])\n",
    "model.load_state_dict(state['model_state_dict'], strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = Cluster(grid_y, grid_x, 1, 1)\n",
    "im = torch.from_numpy(im_numpy[np.newaxis, np.newaxis, ...]).float().cuda()\n",
    "if im.ndimension()==5:\n",
    "    instance_map = []\n",
    "    for z in range(im.shape[2]):\n",
    "        im_z = im[:, :, z, ...]\n",
    "        multiple_y = im_z.shape[2] // 8\n",
    "        multiple_x = im_z.shape[3] // 8\n",
    "\n",
    "        if im_z.shape[2] % 8 != 0:\n",
    "            diff_y = 8 * (multiple_y + 1) - im_z.shape[2]\n",
    "        else:\n",
    "            diff_y = 0\n",
    "        if im_z.shape[3] % 8 != 0:\n",
    "            diff_x = 8 * (multiple_x + 1) - im_z.shape[3]\n",
    "        else:\n",
    "            diff_x = 0\n",
    "        p2d = (diff_x // 2, diff_x - diff_x // 2, diff_y // 2, diff_y - diff_y // 2)\n",
    "\n",
    "        im_z = F.pad(im_z, p2d, \"constant\", 0)\n",
    "        output = model(im_z) # output size is  1 5 Y X \n",
    "        #####################\n",
    "        ##GET SEED MAP######\n",
    "        ####################\n",
    "        # seed_map = torch.sigmoid(output[0, -1, ...]) # seed maps is the last channel of the output \n",
    "        # tifffile.imsave('pathname', seed_map.cpu().detach().numpy()) # replace `pathname` with appropriate \n",
    "                                                                       # location to save `seed_map`\n",
    "        instance_map_z, predictions_z = cluster.cluster(prediction = output[0])\n",
    "\n",
    "        instance_map.append(instance_map_z.unsqueeze(0))\n",
    "       \n",
    "       \n",
    "    instance_map=torch.cat(instance_map).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a new pre-saved color map!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "new_cmp= np.load('../../../cmaps/cmap_60.npy')\n",
    "new_cmp = ListedColormap(new_cmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Feel free to re-run the following cell multiple times to look at random time frames! <br>\n",
    "On the left is the image and the right is the prediction by the trained model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.random.randint(im_numpy.shape[0])\n",
    "print(\"Showing time point = {}\".format(z))\n",
    "visualize_im_pred(im_numpy[z][diff_y // 2: -(diff_y - diff_y // 2), :], instance_map[z], new_cmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EmbedSegEnv",
   "language": "python",
   "name": "embedsegenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
