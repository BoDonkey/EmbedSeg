{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from EmbedSeg.train import begin_training\n",
    "from EmbedSeg.utils.create_dicts import create_dataset_dict, create_model_dict, create_loss_dict, create_configs\n",
    "import torch\n",
    "%matplotlib tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "today = date.today()\n",
    "today = today.strftime(\"%b-%d-%Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the path to `train`, `val` crops and the type of `center` embedding which we would like to train the network for:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train-val images, masks and center-images will be accessed from the path specified by `data_dir` and `project-name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Name chosen as : basel-2020. \n",
      "Train-Val images-masks-center-images will be accessed from : crops\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'crops'\n",
    "project_name = 'basel-2020'\n",
    "center = 'centroid'\n",
    "\n",
    "print(\"Project Name chosen as : {}. \\nTrain-Val images-masks-center-images will be accessed from : {}\".format(project_name, data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spatial Embedding Location chosen as : centroid\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    assert center in {'medoid', 'approximate-medoid', 'centroid'}\n",
    "    print(\"Spatial Embedding Location chosen as : {}\".format(center))\n",
    "except AssertionError as e:\n",
    "    e.args += ('Please specify center as one of : {\"medoid\", \"approximate-medoid\", \"centroid\"}', 42)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify training dataset-related parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some hints: \n",
    "* The `train_size` attribute indicates the number of image-mask paired examples which the network would see in one complete epoch. Ideally this should be the number of `train` image crops. For the `basel-2020` dataset, we obtain ~60000 crops, which leads to a slow computation. Hence, we go for something quicker and set `train_size` to 20000. \n",
    "* The effective batch size is determined as a product of the attributes `train_batch_size` and `virtual_train_batch_multiplier`. For example, one could set a small `batch_size` say equal to 2 (to fit in one's GPU memory), and a large `virtual_train_batch_multiplier` say equal to 8, to get an effective batch size equal to 16. \n",
    "* The `normalization_factor` attribute normalizes the raw images to always be between 0 and 1. Since the phase contrast image intensities are already between 0 and 1, we set `normalization_factor`=1\n",
    "* The `one_hot` attribute should be set to True if the instance image is present in an one-hot encoded style (i.e. object instance is encoded as 1 in its own individual image slice) and False if the instance image is the same dimensions as the raw-image. \n",
    "\n",
    "In the cell after this one, a `train_dataset_dict` dictionary is generated from the parameters specified here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 20000\n",
    "train_batch_size = 128 \n",
    "virtual_train_batch_multiplier = 1 \n",
    "normalization_factor = 1\n",
    "one_hot = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the `train_dataset_dict` dictionary  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`train_dataset_dict` dictionary successfully created with: \n",
      " -- train images accessed from crops/basel-2020/train/images, \n",
      " -- number of images per epoch equal to 20000, \n",
      " -- batch size set at 128, \n",
      " -- virtual batch multiplier set as 1, \n",
      " -- normalization_factor set as 1, \n",
      " -- one_hot set as False, \n"
     ]
    }
   ],
   "source": [
    "train_dataset_dict = create_dataset_dict(data_dir = data_dir, \n",
    "                                         project_name = project_name,  \n",
    "                                         center = center, \n",
    "                                         size = train_size, \n",
    "                                         batch_size = train_batch_size, \n",
    "                                         virtual_batch_multiplier = virtual_train_batch_multiplier, \n",
    "                                         normalization_factor= normalization_factor,\n",
    "                                         one_hot = one_hot,\n",
    "                                         type = 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify validation dataset-related parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some hints:\n",
    "* The size attribute indicates the number of image-mask paired examples which the network would see in one complete epoch. Here, it is recommended to set `val_size` equal to the total number of validation image crops. For example, for the `basel-2020` dataset, we notice ~10000 validation crops, hence we set `val_size = 10000`.\n",
    "* The effective batch size is determined as a product of the attributes `val_batch_size` and `virtual_val_batch_multiplier`. Here at times, it is okay to set a higher effective batch size for the validation dataset than the train dataset, since evaluating on validation data consumes lesser GPU memory.\n",
    "* The `normalization_factor` attribute normalizes the raw images to always be between 0 and 1. \n",
    "* The `one_hot` attribute should be set to True if the instance image is present in an one-hot encoded style (i.e. object instance is encoded as 1 in its own individual image slice) and False if the instance image is the same dimensions as the raw-image. \n",
    "\n",
    "In the cell after this one, a `val_dataset_dict` dictionary is generated from the parameters specified here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 10000\n",
    "val_batch_size = 128\n",
    "virtual_val_batch_multiplier = 1\n",
    "normalization_factor = 1\n",
    "one_hot = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the `val_dataset_dict` dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`val_dataset_dict` dictionary successfully created with: \n",
      " -- val images accessed from crops/basel-2020/val/images, \n",
      " -- number of images per epoch equal to 10000, \n",
      " -- batch size set at 128, \n",
      " -- virtual batch multiplier set as 1, \n",
      " -- normalization_factor set as 1, \n",
      " -- one_hot set as False, \n"
     ]
    }
   ],
   "source": [
    "val_dataset_dict = create_dataset_dict(data_dir = data_dir, \n",
    "                                       project_name = project_name, \n",
    "                                       center = center, \n",
    "                                       size = val_size, \n",
    "                                       batch_size = val_batch_size, \n",
    "                                       virtual_batch_multiplier = virtual_val_batch_multiplier,\n",
    "                                       normalization_factor= normalization_factor,\n",
    "                                       one_hot = one_hot,\n",
    "                                       type ='val',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify model-related parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some hints:\n",
    "* Set the `input_channels` attribute equal to 1 for gray-scale and 3 for `RGB` input images. \n",
    "\n",
    "In the cell after this one, a `model_dataset_dict` dictionary is generated from the parameters specified here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the `model_dict` dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`model_dict` dictionary successfully created with: \n",
      " -- num of classes equal to 1, \n",
      " -- input channels equal to [4, 1], \n",
      " -- name equal to branched_erfnet\n"
     ]
    }
   ],
   "source": [
    "model_dict = create_model_dict(input_channels = input_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the `loss_dict` dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`loss_dict` dictionary successfully created with: \n",
      " -- foreground weight equal to 10, \n",
      " -- w_inst equal to 1, \n",
      " -- w_var equal to 10, \n",
      " -- w_seed equal to 1\n"
     ]
    }
   ],
   "source": [
    "loss_dict = create_loss_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify additional parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some hints:\n",
    "* The `n_epochs` attribute determines how long the training should proceed. In general for reasonable results, you should atleast train for longer than 25 epochs.\n",
    "* The `display` attribute, if set to True, allows you to see the network predictions as the training proceeds. If you would like the `display` to not be `inline`, you could add a new cell after this one and say `%matplotlib tk`, which would pop the visualization as a separate window. \n",
    "* The `display_embedding` attribute, if set to True, allows you to see some sample embedding as the training proceeds. Setting this to False leads to faster training times.\n",
    "* The `save_dir` attribute identifies the location where the checkpoints and loss curve details are saved. \n",
    "* If one wishes to **resume training** from a previous checkpoint, they could point `resume_path` attribute appropriately. For example, one could set `resume_path = './exp/basel-2020/checkpoint.pth'` to resume training from the last checkpoint. \n",
    "* The `grid_y` and `grid_x` attributes should be set to equal or more than the dimensions of the largest evaluation image one wishes to test the trained model on. (Here, we assume that the pixel sizes in the height and width dimension are equal). If you are unsure of the evaluation image sizes at this stage, best to leave these attributes as they are.  \n",
    "* The `one_hot` attribute should be set to True if the instance image is present in an one-hot encoded style (i.e. object instance is encoded as 1 in its own individual image slice) and False if the instance image is the same dimensions as the raw-image. \n",
    "\n",
    "\n",
    "\n",
    "In the cell after this one, a `configs` dictionary is generated from the parameters specified here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 200\n",
    "one_hot = False\n",
    "display = False\n",
    "display_embedding = False\n",
    "save_dir = os.path.join('experiment', project_name+'-'+today)\n",
    "resume_path  = None\n",
    "grid_y = 1024\n",
    "grid_x = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the  `configs` dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`configs` dictionary successfully created with: \n",
      " -- n_epochs equal to 200, \n",
      " -- display equal to False, \n",
      " -- save_dir equal to experiment/basel-2020-Jan-03-2021, \n",
      " -- grid_y equal to 1024, \n",
      " -- grid_x equal to 1024, \n",
      " -- one_hot equal to False, \n"
     ]
    }
   ],
   "source": [
    "configs = create_configs(n_epochs = n_epochs,\n",
    "                         one_hot = one_hot,\n",
    "                         display = display, \n",
    "                         display_embedding = display_embedding,\n",
    "                         resume_path = resume_path, \n",
    "                         save_dir = save_dir, \n",
    "                         grid_y = grid_y, \n",
    "                         grid_x = grid_x,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the next cell would begin the training. \n",
    "\n",
    "If `display` attribute was set to `True` above, then you would see the network predictions at every $n^{th}$ step (equals 5, by default) on training and validation images. \n",
    "\n",
    "Going clockwise from top-left is \n",
    "\n",
    "    * the raw-image which needs to be segmented, \n",
    "    * the corresponding ground truth instance mask, \n",
    "    * the network predicted instance mask, and \n",
    "    * (if display_embedding = True) from each object instance, 5 pixels are randomly selected (indicated with `+`), their embeddings are plotted (indicated with `.`) and the predicted margin for that object is visualized as an axis-aligned ellipse centred on the ground-truth - center (indicated with `x`)  for that object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Training upto 200 epochs would take a while. Each epoch roughly takes around 10 minutes. <br>\n",
    "One can get decent enough results in 10 epochs though!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-D `train` dataloader created! Accessing data from crops/basel-2020/train/\n",
      "Number of images in `train` directory is 67142\n",
      "Number of instances in `train` directory is 67142\n",
      "Number of center images in `train` directory is 67142\n",
      "*************************\n",
      "2-D `val` dataloader created! Accessing data from crops/basel-2020/val/\n",
      "Number of images in `val` directory is 10410\n",
      "Number of instances in `val` directory is 10410\n",
      "Number of center images in `val` directory is 10410\n",
      "*************************\n",
      "Creating branched erfnet with [4, 1] classes\n",
      "Initialize last layer with size:  torch.Size([16, 4, 2, 2])\n",
      "*************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/156 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created spatial emb loss function with: n_sigma: 2, foreground_weight: 10\n",
      "*************************\n",
      "Created logger with keys:  ('train', 'val', 'iou')\n",
      "Starting epoch 0\n",
      "learning rate: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [07:40<00:00,  2.95s/it]\n",
      "100%|██████████| 78/78 [01:55<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> train loss: 0.85\n",
      "===> val loss: 0.63, val iou: 0.82\n",
      "=> saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/156 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "learning rate: 0.0004977494364660346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [09:30<00:00,  3.66s/it]\n",
      "100%|██████████| 78/78 [02:25<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> train loss: 0.44\n",
      "===> val loss: 0.42, val iou: 0.89\n",
      "=> saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/156 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 2\n",
      "learning rate: 0.0004954977417064171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [11:56<00:00,  4.60s/it]\n",
      "100%|██████████| 78/78 [02:50<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> train loss: 0.32\n",
      "===> val loss: 0.31, val iou: 0.90\n",
      "=> saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/156 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 3\n",
      "learning rate: 0.0004932449094349202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [08:36<00:00,  3.31s/it]\n",
      "100%|██████████| 78/78 [01:58<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> train loss: 0.28\n",
      "===> val loss: 0.29, val iou: 0.90\n",
      "=> saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/156 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 4\n",
      "learning rate: 0.0004909909332982877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [08:43<00:00,  3.36s/it]\n",
      "100%|██████████| 78/78 [01:57<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> train loss: 0.25\n",
      "===> val loss: 0.25, val iou: 0.91\n",
      "=> saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/156 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 5\n",
      "learning rate: 0.0004887358068751748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [08:58<00:00,  3.45s/it]\n",
      "100%|██████████| 78/78 [01:57<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> train loss: 0.24\n",
      "===> val loss: 0.23, val iou: 0.92\n",
      "=> saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/156 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 6\n",
      "learning rate: 0.0004864795236750653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [09:06<00:00,  3.50s/it]\n",
      "100%|██████████| 78/78 [02:15<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> train loss: 0.23\n",
      "===> val loss: 0.23, val iou: 0.92\n",
      "=> saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/156 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 7\n",
      "learning rate: 0.00048422207713716544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [11:37<00:00,  4.47s/it]\n",
      "100%|██████████| 78/78 [02:51<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> train loss: 0.21\n",
      "===> val loss: 0.21, val iou: 0.92\n",
      "=> saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/156 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 8\n",
      "learning rate: 0.00048196346062927547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 55/156 [03:28<07:23,  4.39s/it]"
     ]
    }
   ],
   "source": [
    "begin_training(train_dataset_dict, val_dataset_dict, model_dict, loss_dict, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EmbedSegEnv",
   "language": "python",
   "name": "embedsegenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "47.7167px",
    "left": "1689.05px",
    "top": "111.133px",
    "width": "158.95px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
