{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import tifffile\n",
    "import numpy as np\n",
    "import os\n",
    "from EmbedSeg.utils.preprocess_data import extract_data, split_train_val, split_train_test\n",
    "from EmbedSeg.utils.generate_crops import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images and corresponding masks are downloaded from an external url, specified by `zip_url` to the path specified by the variables `data_dir` and `project_dir`. The following structure is generated after executing the `extract_data` method below:\n",
    "\n",
    "```\n",
    "data\n",
    "└───basel-2020\n",
    "    |───download\n",
    "        |───train\n",
    "            └───images\n",
    "            └───masks\n",
    "    |───basel-2020.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "The `basel-2020` dataset is 1.25 GB large. Downloading may take a while!<br>\n",
    "Here, we have pre-saved phase-contrast channel images and corresponding masks in the `images` and `masks` sub-directories respectively.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../../data'\n",
    "project_name = 'basel-2020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "extract_data(\n",
    "    zip_url = 'https://github.com/juglab/EmbedSeg/releases/download/v1.0/basel-2020.zip',\n",
    "    data_dir = data_dir,\n",
    "    project_name = project_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into train  \\& test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the train-test data partition doesn't exist by itself in the original data, we can execute the following cell to reserve some data (10 \\% by default) as evaluation or test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_train_test(\n",
    "    data_dir = data_dir,\n",
    "    project_name = project_name, \n",
    "    train_test_name = 'train',\n",
    "    subset = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into `train`, `val` \\& `test`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we would like to reserve a small fraction (15 % by default) of the provided train dataset as validation data. Here, in case you would like to repeat multiple experiments with the same partition, you may continue and press <kbd>Shift</kbd> + <kbd>Enter</kbd> on the next cell - but in case, you would like different partitions each time, please add the `seed` attribute equal to a different integer (For example, \n",
    "```\n",
    "split_train_val(\n",
    "data_dir = data_dir, \n",
    "project_name = project_name, \n",
    "train_val_name = 'train', \n",
    "subset = 0.15,\n",
    "seed = 1000)\n",
    "```\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_train_val(\n",
    "    data_dir = data_dir,\n",
    "    project_name = project_name, \n",
    "    train_val_name = 'train',\n",
    "    subset = 0.15, \n",
    "    mode = 'TYX')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify desired centre location for spatial embedding of pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interior pixels of an object instance can either be embedded at the `centroid` (evaluated in $\\mathcal{O(n)}$ operations, where $\\mathcal{n}$ is the number of pixels in an object instance), or the `approximate-medoid` (also evaluated in $\\mathcal{O(n)}$ operations) or the `medoid` (evaluated in $\\mathcal{O(n^{2})}$ operations). Please note that evaluating `medoid` of the instances could be slow especially if you choose a large `crop_size` later: in such a scenario, a quicker alternative is opting for the `approximate-medoid` option, which gives comparable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spatial Embedding Location chosen as : centroid\n"
     ]
    }
   ],
   "source": [
    "center = 'centroid'\n",
    "try:\n",
    "    assert center in {'medoid', 'approximate-medoid', 'centroid'}\n",
    "    print(\"Spatial Embedding Location chosen as : {}\".format(center))\n",
    "except AssertionError as e:\n",
    "    e.args += ('Please specify center as one of : {\"medoid\", \"approximate-medoid\", \"centroid\"}', 42)\n",
    "    raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify cropping configuration parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images and the corresponding masks are cropped into patches centred around an object instance, which are pre-saved prior to initiating the training. **Run the following two cells twice** - first time set `data_subset = 'train'` and the second time set `data_subset = 'val'`.  Note that the cropped images, masks and center-images would be saved at the path specified by `crops_dir`. Please set `one_hot = True` in case the instances are encoded in a one-hot style. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "crops_dir = 'crops'\n",
    "data_subset = 'train' \n",
    "crop_size_y = 200\n",
    "crop_size_x = 64\n",
    "norm = False\n",
    "one_hot = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Crops\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "The cropped images and masks are saved at the same-location as the example notebooks. <br>\n",
    "Generating the crops would take a little while!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/15842 [00:00<04:26, 59.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new directory : crops/basel-2020/train/images/\n",
      "Created new directory : crops/basel-2020/train/masks/\n",
      "Created new directory : crops/basel-2020/train/center-centroid/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15842/15842 [27:24<00:00,  9.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropping of images, instances and centre_images for data_subset = `train` done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "image_dir = os.path.join(data_dir, project_name, data_subset, 'images')\n",
    "instance_dir = os.path.join(data_dir, project_name, data_subset, 'masks')\n",
    "image_names = sorted(glob(os.path.join(image_dir, '*.tif'))) \n",
    "instance_names = sorted(glob(os.path.join(instance_dir, '*.tif')))  \n",
    "for i in tqdm(np.arange(len(image_names))):\n",
    "    if one_hot:\n",
    "        process_one_hot(image_names[i], instance_names[i], os.path.join(crops_dir, project_name), data_subset, crop_size, center, one_hot = one_hot)\n",
    "    else:\n",
    "        process(image_names[i], instance_names[i], os.path.join(crops_dir, project_name), data_subset, crop_size_y = crop_size_y, crop_size_x = crop_size_x, center=center, one_hot=one_hot, norm = norm)\n",
    "print(\"Cropping of images, instances and centre_images for data_subset = `{}` done!\".format(data_subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EmbedSegEnv",
   "language": "python",
   "name": "embedsegenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
